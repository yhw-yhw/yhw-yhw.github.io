<!DOCTYPE HTML>

<head>
  <!-- <title>Show/Hide Elements</title> -->
  <link rel="stylesheet" type="text/css" href="styles.css">
</head>
<link rel="stylesheet" href="styles.css">




<!-- <script src="https://code.jquery.com/jquery-3.1.1.min.js" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js"
        integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js"
        integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
<script type="text/javascript" src="static/js/jquery.color.min.js"></script>
<script type="text/javascript"> -->


<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hongwei yi</title>

  <meta name="author" content="Hongwei Yi">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>


<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <!-- <td style="padding:2.5%;width:35%;max-width:40%">
              <a href="images/hongwei.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/hongwei.png" class="hoverZoomLink"></a>
            </td> -->
                <td style="padding: 2.5%; width: 35%; max-width: 40%; border-radius: 50%;">
                  <a href="images/hongwei.JPG">
                    <img style="width: 100%; max-width: 100%; border-radius: 50%;" alt="profile photo"
                      src="images/hongwei.JPG" class="hoverZoomLink">
                  </a>
                </td>


                <td style="padding:2.5%;width:35%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Hongwei Yi</name>
                  </p>

                  <p style="text-align:center">
                    <a href="files/hwyi_cv.pdf">CV</a> &nbsp/&nbsp
                    <a href="mailto:hongwei.yi@tuebingen.mpg.de">Email</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/%E9%B8%BF%E4%BC%9F-%E6%98%93-758590195/">LinkedIn</a>
                    &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=ocMf7fQAAAAJ&hl=zh-CN">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/HongweiYi2">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/yhw-yhw">Github</a>

                    <!-- &nbsp/&nbsp  -->
                    <!-- <a href="https://www.dropbox.com/s/gpb21lc9pgq66yx/hwyi_cv.pdf?dl=0">CV</a> -->
                  </p>
                </td>

                <td style="padding:2.5%;width:35%;vertical-align:middle">
                  <p style="font-size: 20px"> Research Topics: </p>
                  <p style="text-align:left">
                    Generative Modeling <br>
                    Human-Scene Interaction <br>
                    Motion Generation <br>
                    Avatar Generation <br>
                    Scene Understanding <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <!-- personal description -->
          <p>I am a third-year PhD student (2020.09-) at the <a href="https://ps.is.tuebingen.mpg.de">Perceiving Systems
              Department</a> of <a href="https://is.mpg.de/">Max Planck Institute for Intelligent Systems</a>, jointly
            supervised by <a href="https://ps.is.tuebingen.mpg.de/person/black">Michael J. Black</a> and <a
              href="https://vlg.inf.ethz.ch/people/person-detail.siyutang.html">Siyu Tang</a>, and work closely with <a
              href="https://justusthies.github.io/"> Justus Thies</a>.
            <!-- Before that, I achieved my M.S. in computer applied technology from <a
              href="http://english.pku.edu.cn/">Peking University</a> in 2020.07 and earned my B.S. in computer science
            and technology from <a href="https://www.bupt.edu.cn/">Beijing University of Posts and
              Telecommunications</a> in 2017.07. -->
              Before that, I achieved my M.S. from <a
              href="http://english.pku.edu.cn/">Peking University</a> and earned my B.S. from <a href="https://www.bupt.edu.cn/">Beijing University of Posts and
              Telecommunications</a>.
          </p>

          <p>
            My main research is mainly drived by the goal of developing autonomous digital avatars which can interact
            with real humans in the physical/virtual world <span style="font-weight: bold;">(AR/VR)</span>.
            My research has mainly focused on capturing <span style="font-weight: bold;">human-scene interaction</span>
            from RGB videos through computer vision and machine learning;
            My work further on how to generate plausible human-scene interaction, to build the loop between scenes from
            humans (<span style="font-weight: bold;">scene generation</span>) and humans from scenes (<span
              style="font-weight: bold;">motion generation</span>);
            To empower digital avatar with expressive communication ability, my work has extended to creating <span
              style="font-weight: bold;">high-fidelity animatable expressive avatar</span> from single image or text
            based on implicit function and LLM; and expressive holistic body motion generation from audio through
            generative modeling.
            More broadly, I am super interested in <span style="font-weight: bold;">revolutionalizing the cinema and creating immersive telecommunication with
              autonomous life-like digital avatar</span> creation technology.
          </p>
          
          <p>
            <strong>I will be on job market in 2024, and am actively looking for research internship opportunity in Spring 2024, and full-time job in big companies or start-ups after Summer 2024.</strong>
          </p>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>News</heading>
                <ul>
                  <li><strong>08/2023</strong> <a href="https://tada.is.tue.mpg.de">TADA!</a> and <a
                      href="https://huangyangyi.github.io/TeCH/">TeCH</a> come out in arXiv.
                  <li><strong>07/2023</strong> Our papers <a href=""><strong>ELICIT</strong></a> and <a
                      href=""><strong>DECO</strong></a> are accepted to ICCV 2023!
                  <li><strong>07/2023</strong> Our paper <a href="https://arxiv.org/abs/2306.16736"><strong>GRAMMER</strong></a> is
                    accepeted in ACMMM 2023!</a></li>
                  <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
                  <div id="old_news" style="display: none;">

                    <li><strong>05/2022</strong> Invited to give a talk on at <a href="http://www.csig3dv.net/">China3DV
                        2023</a> (<a href="">Slides</a>). </li>
                  </div>
                  </div>
                </ul>
              </td>
            </tr>


            <!-- </tbody></table> -->
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <heading>Publication</heading>

                    (
                    <a href="#" id="showToShow">Highlighted Papers</a> /
                    <a href="#" id="showAll">All Papers</a>
                    )

                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"
              , id="pub_lists">
              <tbody>
                
              <tr>

                <td style="padding:20px;width:25%;vertical-align:middle">
                <video autoplay loop muted width="200" height="">
                  <source src="images/poco.mp4" type="video/mp4" />
                 </video>
                </td>
                
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>POCO: 3D Pose and Shape Estimation using Confidence</papertitle>
                  </a>
                  <br>
                  <a href="https://ps.is.mpg.de/person/sdwivedi">Sai Kumar Dwivedi</a>,
                  <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>,
                  <strong>Hongwei Yi</strong>,
                  <a href="https://ps.is.mpg.de/person/black">Michael J. Black</a>,
                  <a href="https://ps.is.mpg.de/person/dtzionas">Dimitris Tzoinas</a>,
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                  <!-- / -->
                  <a href="">code</a>
                  /
                  <a href="https://arxiv.org/abs/2308.12965">arXiv</a>
                  /
                  <a href="">video</a>
                  /
                  <a href="https://poco.is.tue.mpg.de/">project</a>
                  <p>
                    Learning HPS (human pose shape) estimation with confidence (hot: not confident, cold: confident).
                  </p>
                  
                  <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                </td>

              </tr>
              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <video autoplay loop muted width="200" height="">
                    <source src="images/proxycap.mp4" type="video/mp4" />
                   </video>
                  </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>Real-time Monocular Full-body Capture in World Space via Sequential
                      Proxy-to-Motion Learning</papertitle>
                  </a>
                  <br>
                  <a href="https://zhangyux15.github.io/">Yuxiang Zhang</a>,
                  <a href="https://hongwenzhang.github.io/">Hongwen Zhang</a>,
                  <a href="https://www.researchgate.net/profile/Liangxiao-Hu">Liangxiao Hu</a>,
                  <strong>Hongwei Yi</strong>,
                  <a href="https://scholar.google.com/citations?user=hMNsT8sAAAAJ&hl=zh-CN">Shengping Zhang</a>,
                  <a href="https://liuyebin.com">Yebin Liu</a>
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                  <!-- / -->
                  <a href="">code</a>
                  /
                  <a href="https://arxiv.org/pdf/2307.01200.pdf">arXiv</a>
                  /
                  <a href="https://liuyebin.com/proxycap/">project</a>
                  <p>
                    Real-time motion capture from RGB video by learning the mapping from 2D detected joints to 3D body mesh in world space.
                  </p>
                  <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                </td>
              </tr>
              <tr class="elements-to-show">

                

                <td style="padding:20px;width:25%;vertical-align:middle">
                <video autoplay loop muted width="200" height="">
                  <source src="images/tada.mp4" type="video/mp4" />
                 </video>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>TADA! Text to Animatable Digital Avatars&nbsp</papertitle>
                  </a>
                  <br>
                  <a href="https://github.com/TingtingLiao">Tingting Liao*</a>,
                  <strong>Hongwei Yi*</strong>,
                  <a href="https://xiuyuliang.cn">Yuliang Xiu</a>,
                  <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
                  <a href="https://huangyangyi.github.io/">Yangyi Huang</a>,
                  <a href="https://justusthies.github.io/">Justus Thies</a>,
                  <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>
                  (* denotes equal contribution)
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                  <!-- / -->
                  <a href="https://github.com/TingtingLiao/TADA">code</a>
                  /
                  <a href="https://arxiv.org/abs/2308.10899">arXiv</a>
                  /
                  <a href="https://youtu.be/w5rdcPQWktE">video</a>
                  /
                  <a href="https://tada.is.tue.mpg.de/">project</a>
                  <p>
                    text to animatable full-body avatars based on T2I (I: image; T: text) model.
                  </p>
                  
                  <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                </td>

              </tr>

                <tr class="elements-to-show">

                  

                  <td style="padding:20px;width:25%;vertical-align:middle">
                   <video autoplay loop muted width="200">
                    <source src="./images/tech.mp4" type=video/mp4>
                  </video>
                  </td> 

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>TeCH: Text-guided Reconstruction of Lifelike Clothed
                        Humans&nbsp</papertitle>
                    </a>
                    <br>
                    <a href="https://github.com/huangyangyi">Yangyi Huang*</a>,
                    <strong>Hongwei Yi*</strong>,
                    <a href="https://xiuyuliang.cn">Yuliang Xiu*</a>,
                    <a href="https://github.com/TingtingLiao">Tingting Liao</a>,
                    <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
                    <a href="http://www.cad.zju.edu.cn/home/dengcai/">Deng Cai</a>,
                    <a href="https://justusthies.github.io/">Justus Thies</a>
                    (* denotes equal contribution)
                    <br>
                    <em>arXiv</em>, 2023
                    <br>
                    <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                    <!-- / -->
                    <a href="https://github.com/huangyangyi/ELICIT">code</a>
                    /
                    <a href="https://arxiv.org/abs/2212.02469">arXiv</a>
                    /
                    <a href="https://elicit3d.github.io/">project</a>
                    <p>
                      A high-fidelity mesh-based full-body avatar reconstruction from a single image with text assistancy from I2T and T2I models.
                    </p>
                    <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                  </td>
                  

                  <tr class="">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                      <video autoplay loop muted width="200">
                        <source src="images/grammer.mp4" type=video/mp4>
                      </video>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="">
                        <papertitle>GraMMaR: Ground-aware Motion Model for 3D Human Motion Reconstruction</papertitle>
                      </a>
                      <br>
                      <a href="">Sihan Ma</a>,
                      <a href="https://qiongcao.github.io/">Qiong Cao</a>,
                      <strong>Hongwei Yi</strong>,
                      <a href="">Jing Zhang</a>,
                      <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">Dacheng Tao</a>
                      <br>
                      <em>ACMMM</em>, 2023
                      <br>
                      <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                      <!-- / -->
                      <a href="https://github.com/xymsh/GraMMaR">code</a>
                      /
                      <a href="https://arxiv.org/abs/2306.16736">arXiv</a>
                      /
                      <a href="">project</a>
                      <p>
                        Learning denser human-ground contact motion prior to improve motion reconstruction from monocular RGB video.
                      </p>
                      <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                    </td>
                  </tr>
                
                  <tr class="elements-to-show">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                      <img src="images/deco_teaser.png" alt="prl" width="200" height="">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="">
                        <papertitle>DECO: Dense Estimation of 3D Human-Scene COntact in the Wild</papertitle>
                      </a>
                      <br>
                  
                      <a href="https://sha2nkt.github.io/">Shashank tripathi*</a>,
                      <a href="https://ac5113.github.io/">Agniv Chatterjee*</a>,
                      <a href="https://is.mpg.de/person/jpassy">Jean-Claude Passy</a>,
                      <strong>Hongwei Yi</strong>,
                      <a href="https://ps.is.mpg.de/person/dtzionas">Dimitrios Tzionas</a>,
                      <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>
                      (* denotes equal contribution)
                      <br>
                      <em>ICCV</em>, Oral, 2023
                      <br>
                      <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                      <!-- / -->
                      <a href="https://github.com/sha2nkt/deco">code</a>
                      /
                      <a href="">arXiv</a>
                      /
                      <a href="https://deco.is.tue.mpg.de/">project</a>
                      <p>
                        Estimate dense human-scene contact from in-the-wild images.
                      </p>
                      <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                    </td>
                  </tr>

                <tr class="elements-to-show">

                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/elicit_teaser.gif" alt="prl" width="200" height="">
                  </td>

                  <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                  <video autoplay loop width="200" height="" controls="controls">
                    <source src="images/elicit3_teaser.mp4" type="video/mp4" />
                   </video>
                  </td> -->

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>One-shot Implicit Animatable Avatars with
                        Model-based Priors</papertitle>
                    </a>
                    <br>
                    <a href="https://github.com/huangyangyi">Yangyi Huang*</a>,
                    <strong>Hongwei Yi*</strong>,
                    <a href="https://wyliu.com/">Weiyang Liu</a>,
                    <a href="https://haofanwang.github.io/">Haofan Wang</a>,
                    <a href="https://scholar.google.com/citations?user=AqDe35sAAAAJ">Boxi Wu</a>,
                    <a href="https://www.wenxiaowang.com/">Wenxiao Wang</a>,
                    <a href="https://scholar.google.com/citations?user=Zmvq4KYAAAAJ">Binbin Lin</a>,
                    <a href="https://www.linkedin.com/in/%E5%BE%B7%E5%85%B5-%E5%BC%A0-a6451aa9?">Debing Zhang</a>,
                    <a href="http://www.cad.zju.edu.cn/home/dengcai/">Deng Cai</a>
                    (* denotes equal contribution)
                    <br>
                    <em>ICCV</em>, 2023
                    <br>
                    <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                    <!-- / -->
                    <a href="https://github.com/huangyangyi/ELICIT">code</a>
                    /
                    <a href="https://arxiv.org/abs/2212.02469">arXiv</a>
                    /
                    <a href="https://elicit3d.github.io/">project</a>
                    <p>
                      A animatable NeRF-based avatar reconstruction from a single image.
                    </p>
                    <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                  </td>

                </tr>

                <tr class="elements-to-show">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/mime_teaser.gif" alt="prl" width="200" height="">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>MIME: Human-Aware 3D Scene Generation</papertitle>
                    </a>
                    <br>
                    <strong>Hongwei Yi</strong>,
                    <a href="https://is.mpg.de/person/chuang2">Chun-Hao P. Huang</a>,
                    <a href="https://sha2nkt.github.io/">Shashank tripathi</a>,
                    <a href="https://ps.is.mpg.de/person/lhering">Lea Hering</a>,

                    <a href="https://justusthies.github.io/openings/">Justus Thies</a>,
                    <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>
                    <br>
                    <em>CVPR</em>, 2023
                    <br>
                    <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                    <!-- / -->
                    <a href="https://github.com/yhw-yhw/MIME">code</a>
                    /
                    <a href="https://arxiv.org/pdf/2212.04360.pdf">arXiv</a>
                    /
                    <a href="https://mime.is.tue.mpg.de/">project</a>
                    <p>
                      Generate 3D scenes from input humans for creating large-scale human-scene interaction datasets. 
                    </p>
                    <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                  </td>
                </tr>

                <tr class="elements-to-show">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <video autoplay loop muted width="200">
                      <source src="./images/talkshow.mp4" type=video/mp4>
                    </video>
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>Generating Holistic 3D Human Motion from Speech</papertitle>
                    </a>
                    <br>
                    <strong>Hongwei Yi*</strong>,
                    <a href="https://github.com/lithiumice">Hualin Liang*</a>,
                    <a href="https://github.com/feifeifeiliu">Yifei Liu*</a>,
                    <a href="https://qiongcao.github.io/">Qiong Cao†</a>,
                    <a href="https://is.mpg.de/person/ydwen">Yandong Wen</a>,

                    <a href="https://ps.is.mpg.de/person/tbolkart">Timo Bolkart</a>,
                    <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">Dacheng
                      Tao</a>,
                    <a href="https://ps.is.mpg.de/~black">Michael J. Black†</a>
                    (* denotes equal contribution, † denotes joint corresponding authors)
                    <br>
                    <em>CVPR</em>, 2023
                    <br>
                    <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                    <!-- / -->
                    <a href="https://github.com/yhw-yhw/TalkSHOW">TalkSHOW code</a>
                    /
                    <a href="https://github.com/yhw-yhw/SHOW">SHOW code</a>
                    /
                    <a href="https://arxiv.org/abs/2212.04420">arXiv</a>
                    /
                    <a href="https://talkshow.is.tue.mpg.de/">project</a>
                    <p>
                      Empowering expressive digital human with communication abilitity: audio to holistic body motion generation, including hands, body, and face.
                    </p>
                    <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                  </td>
                </tr>

                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/SLOPER.png" alt="prl" width="200" height="">
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>SLOPER4D: A Scene-Aware Dataset For Global 4D Human Pose Estimation In Urban
                        Environments</papertitle>
                    </a>
                    <br>
                    <a href="https://scholar.google.com/citations?user=nn1S_4EAAAAJ&hl=zh-CN">Yudi Dai</a>,
                    <a href="">Yitai Lin</a>,
                    <a href="">Xiping Lin</a>,
                    <a href="https://scsc.xmu.edu.cn/clwen/clwen.html">Chenglu Wen</a>,
                    <a href="https://www.xu-lan.com/">Lan Xu</a>,
                    <strong>Hongwei Yi</strong>,
                    <a href="https://asc.xmu.edu.cn/t/shensiqi">Siqi Shen</a>,
                    <a href="https://yuexinma.me/">Yuexin Ma</a>,
                    <a href="https://chwang.xmu.edu.cn/">Cheng Wang</a>
                    <br>
                    <em>CVPR</em>, 2023
                    <br>
                    <a href="">code</a>
                    /
                    <a href="">arXiv</a>
                    /
                    <a href="">project</a>
                    <p>
                      Lidar+first-view camera for global motion capturing in the large urban environments.
                    </p>
                  </td>
                </tr>


                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/car.gif" alt="prl" width="200" height="">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>High-fidelity Clothed Avatar Reconstruction from a Single Image</papertitle>
                    </a>
                    <br>
                    <a href="">Tingting Liao</a>,
                    <a href="">Xiaomei Zhang</a>,
                    <a href="https://xiuyuliang.cn/">Yuliang Xiu</a>,
                    <strong>Hongwei Yi</strong>,
                    <a href="">Xudong Liu</a>,
                    <a href="https://scholar.google.com/citations?user=Nut-uvoAAAAJ&hl=zh-CN">Guo-Jun Qi</a>,
                    <a href="">Yong Zhang</a>,
                    <a href="">Xuan Wang</a>,
                    <a href="https://xiangyuzhu-open.github.io/homepage/">Xiangyu Zhu</a>,
                    <a href="https://scholar.google.com/citations?user=cuJ3QG8AAAAJ&hl=zh-CN">Zhen Lei</a>
                    <br>
                    <em>CVPR</em>, 2023
                    <br>
                    <a href="">code</a>
                    /
                    <a href="">arXiv</a>
                    /
                    <a href="">project</a>
                    <p></p>
                  </td>
                </tr>


                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/NerfLoc.png" alt="prl" width="200" height="">
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>NeRF-Loc: Transformer-Based Object Localization Within Neural Radiance Fields
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://decayale.github.io/">Jiankai Sun*</a></span>,
                    <a href="https://decayale.github.io/">Yan Xu*</a></span>,
                    <a href="https://dingmyu.github.io/">Mingyu Ding</a></span>,
                    <strong>Hongwei Yi</strong>,
                    <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a></span>,
                    <a href="https://www.cs.unc.edu/~zlj/">Liangjun Zhang</a></span>,
                    <a href="https://profiles.stanford.edu/mac-schwager">Mac Schwager</a></span>
                    (* denotes equal contribution)
                    <br>
                    <em>RA-L</em>, 2023
                    <br>
                    <a href="">code</a>
                    /
                    <a href="https://arxiv.org/abs/2209.12068">Arxiv</a>
                    /
                    <a href="">project</a>
                    <p></p>
                  </td>
                </tr>

                <tr class="elements-to-show">

                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <video autoplay loop muted width="200">
                      <source src="./images/mover.mp4" type=video/mp4>
                    </video>
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>Human-Aware Object Placement for Visual Environment Reconstruction</papertitle>
                    </a>
                    <br>
                    <strong>Hongwei Yi</strong>,
                    <a href="https://is.mpg.de/person/chuang2" target="_blank" rel="noopener">Chun-Hao P. Huang</a>,
                    <a href="https://ps.is.mpg.de/~dtzionas" target="_blank" rel="noopener">Dimitrios Tzionas</a>,
                    <a href="https://ps.is.mpg.de/person/mkocabas" target="_blank" rel="noopener">Muhammed Kocabas</a>,
                    <a href="https://ps.is.mpg.de/person/mhassan" target="_blank" rel="noopener">Mohamed Hassan</a>,
                    <a href="https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html"
                      target="_blank" rel="noopener">Siyu Tang</a>,
                    <a href="https://justusthies.github.io/openings/" target="_blank" rel="noopener">Justus Thies</a>,
                    <a href="https://ps.is.mpg.de/~black" target="_blank" rel="noopener">Michael J. Black</a>
                    <br>
                    <em> CVPR </em>, 2022
                    <br>
                    <a href="https://mover.is.tue.mpg.de/">project page</a>
                    /
                    <a href="https://www.youtube.com/watch?v=u6mx2y1c1NE&t=5s">demo video</a>
                    <!-- / -->
                    <!-- <a href="https://github.com/yhw-yhw/PVAMVSNet">code</a> -->
                    /
                    <a href="https://arxiv.org/pdf/2203.03609.pdf">arXiv</a>
                    /
                    <!-- <a href="https://youtu.be/zBSH-k9GbV4">video</a> -->
                    <p>
                      Human and 3D scene joint reconstruction from a monocular RGB video.
                    </p>
                  </td>
                </tr>

                <tr class="elements-to-show">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <video autoplay loop muted width="200">
                      <source src="./images/rich.mp4" type=video/mp4>
                    </video>
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>Capturing and Inferring Dense Full-Body
                        Human-Scene Contact</papertitle>
                    </a>
                    <br>
                    <a href="https://ps.is.tuebingen.mpg.de/employees/chuang2" target="_blank" rel="noopener">Chun-Hao
                      P. Huang</a>,
                    <strong>Hongwei Yi</strong>,
                    <a href="https://ps.is.mpg.de/person/mhoeschle" target="_blank" rel="noopener">Markus Höschle</a>,
                    <a href="https://is.mpg.de/person/msafroshkin" target="_blank" rel="noopener">Matvey Safroshkin</a>,
                    <a href="https://ps.is.mpg.de/person/talexiadis" target="_blank" rel="noopener">Tsvetelina
                      Alexiadis</a>,
                    <a href="https://is.mpg.de/de/people/senya" target="_blank" rel="noopener">Senya Polikovsky</a>,
                    <a href="https://www.cs.middlebury.edu/~schar/" target="_blank" rel="noopener">Daniel
                      Scharstein</a>,
                    <a href="https://ps.is.tuebingen.mpg.de/employees/black" target="_blank" rel="noopener">Michael J.
                      Black</a>
                    <br>
                    <em> CVPR </em>, 2022
                    <br>
                    <a href="https://rich.is.tue.mpg.de/">project page</a>
                    <!-- / -->
                    <!-- <a href="https://www.youtube.com/watch?v=u6mx2y1c1NE&t=5s">demo video</a> -->
                    <!-- / -->
                    <!-- <a href="https://github.com/yhw-yhw/PVAMVSNet">code</a> -->
                    <!-- / -->
                    <!-- <a href="https://arxiv.org/pdf/2203.03609.pdf">arXiv</a> -->
                    <!-- / -->
                    <!-- <a href="https://youtu.be/zBSH-k9GbV4">video</a> -->
                    <p>
                      A multiview camera human motion captured dataset with 3D scanned scenes in outdoor environments.
                    </p>
                    <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                  </td>
                </tr>



                <tr class="elements-to-show">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/pvamvsnet.png" alt="prl" width="200" height="">
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation</papertitle>
                    </a>
                    <br>
                    <strong>Hongwei Yi*</strong>,
                    <a href="https://www.researchgate.net/profile/Zizhuang-Wei">Zizhuang Wei*</a>,
                    <a href="https://dingmyu.github.io/">Mingyu Ding</a>,
                    <a href="https://scholar.google.com/citations?user=o41-Nj8AAAAJ&hl=ja">Runze Zhang</a>,
                    <a href="https://eecs.pku.edu.cn/info/1502/6740.htm">Yisong Chen</a>,
                    <a href="https://eecs.pku.edu.cn/info/1502/6725.htm">Guoping Wang</a>,
                    <a href="http://scholar.google.com/citations?user=nFhLmFkAAAAJ&hl=zh-CN">Yu-Wing Tai</a>
                    (* denotes equal contribution)
                    <br>
                    <em>ECCV</em>, 2020
                    <br>
                    <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                    <!-- / -->
                    <a href="https://github.com/yhw-yhw/PVAMVSNet">code</a>
                    /
                    <a href="https://arxiv.org/pdf/1912.03001.pdf">arXiv</a>
                    /
                    <!-- <a href="https://youtu.be/zBSH-k9GbV4">video</a> -->
                    <p></p>
                    <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                  </td>
                </tr>


                <tr class="elements-to-show">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/d2hcmvsnet.png" alt="prl" width="200" height="">
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>Dense Hybrid Recurrent Multi-view Stereo Net with Dynamic Consistency Checking
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://openreview.net/profile?id=~Jianfeng_Yan1">Jianfeng Yan*</a>,
                    <strong>Hongwei Yi*</strong>,
                    <a href="https://www.researchgate.net/profile/Zizhuang-Wei">Zizhuang Wei*</a>,
                    <a href="https://dingmyu.github.io/">Mingyu Ding</a>,
                    <a href="https://pratulsrinivasan.github.io/">Runze Zhang</a>,
                    <a href="https://eecs.pku.edu.cn/info/1502/6740.htm">Yisong Chen</a>,
                    <a href="https://eecs.pku.edu.cn/info/1502/6725.htm">Guoping Wang</a>,
                    <a href="http://scholar.google.com/citations?user=nFhLmFkAAAAJ&hl=zh-CN">Yu-Wing Tai</a>
                    (* denotes equal contribution)
                    <br>
                    <em>ECCV</em>, 2020
                    <br>
                    <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                    <!-- / -->
                    <a href="https://github.com/yhw-yhw/D2HC-RMVSNet">code</a>
                    /
                    <a href="https://arxiv.org/abs/2007.10872">arXiv</a>
                    /
                    <!-- <a href="https://youtu.be/zBSH-k9GbV4">video</a> -->
                    <p></p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/segvoxelnet.png" alt="prl" width="200" height="160">
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle
                        Detection from Point Cloud</papertitle>
                    </a>
                    <br>
                    <strong>Hongwei Yi</strong>,
                    <a href="https://shishaoshuai.com/">Shaoshuai Shi</a>,
                    <a href="https://dingmyu.github.io/">Mingyu Ding</a>,
                    <a href="https://scholar.google.com.hk/citations?user=726MCb8AAAAJ&hl=en">Jiankai Sun</a>,
                    <a href="https://scholar.google.com/citations?user=Hhpry4kAAAAJ&hl=en">Kui Xu</a>,
                    <a href="https://scholar.google.com/citations?user=i35tdbMAAAAJ&hl=zh-CN">Hui Zhou</a>,
                    <a href="https://wang-zhe.me/">Zhe Wang</a>,
                    <a href="https://eecs.pku.edu.cn/info/1502/6732.htm">Sheng Li</a>,
                    <a href="https://eecs.pku.edu.cn/info/1502/6725.htm">Guoping Wang</a>
                    <br>
                    <em>ICRA</em>, 2020
                    <br>
                    <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                    <!-- / -->
                    <a href="https://arxiv.org/abs/2002.05316">arXiv</a>
                    /
                    <!-- <a href="https://youtu.be/zBSH-k9GbV4">video</a> -->
                    <p></p>
                    <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                  </td>
                </tr>

                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/arieal_seman.png" alt="prl" width="200" height="160">
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>Semantic 3D Reconstruction with Learning MVS and 2D Segmentation of Aerial Images
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://www.vis.uni-stuttgart.de/institut/team/Wang-00053/">Yao Wang</a>,
                    <a href="https://www.researchgate.net/profile/Zizhuang-Wei">Zizhuang wei</a>,
                    <strong>Hongwei Yi</strong>,
                    <a href="https://eecs.pku.edu.cn/info/1502/6740.htm">Yisong Chen</a>,
                    <a href="https://eecs.pku.edu.cn/info/1502/6725.htm">Guoping Wang</a>
                    <br>
                    <em>Applied Sciences</em>, 2020
                    <br>
                    <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                    <!-- / -->
                    <a href="https://www.mdpi.com/2076-3417/10/4/1275/pdf">arXiv</a>
                    /
                    <!-- <a href="https://youtu.be/zBSH-k9GbV4">video</a> -->
                    <p></p>
                    <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                  </td>
                </tr>

                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/depth_wise_3DOD.png" alt="prl" width="200" height="160">
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>Learning Depth-Guided Convolutions for Monocular 3D Object</papertitle>
                    </a>
                    <br>
                    <a href="https://dingmyu.github.io/">Mingyu Ding</a>,
                    <a href="https://www.researchgate.net/profile/Yuqi-Huo-3">Yuqi Huo</a>,
                    <strong>Hongwei Yi</strong>,
                    <a href="https://wang-zhe.me/">Zhe Wang</a>,
                    <a href="https://shijianping.me/">Jianping Shi</a>,
                    <a href="https://sites.google.com/site/zhiwulu/">Zhiwu Lu</a>,
                    <a href="http://luoping.me/">Ping Luo</a>
                    <br>
                    <em>CVPR</em>, 2020
                    <br>
                    <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                    <!-- / -->
                    <a
                      href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Ding_Learning_Depth-Guided_Convolutions_for_Monocular_3D_Object_Detection_CVPR_2020_paper.pdf">arXiv</a>
                    /
                    <!-- <a href="https://youtu.be/zBSH-k9GbV4">video</a> -->
                    <p></p>
                    <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                  </td>
                </tr>

                <tr class="elements-to-show">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/MMFace.png" alt="prl" width="200" height="160">
                  </td>

                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="">
                      <papertitle>MMFace: A multi-metric regression network for unconstrained face reconstruction
                      </papertitle>
                    </a>
                    <br>
                    <strong>Hongwei Yi</strong>,
                    <a href="https://scholar.google.com/citations?user=WDJL3gYAAAAJ&hl=zh-CN">Chen Li</a>,
                    <a href="https://scholar.google.co.uk/citations?user=JYtbNBsAAAAJ&hl=en">Qiong Cao</a>,
                    <a href="http://scholar.google.com/citations?user=PeMuphgAAAAJ&hl=zh-CN">Xiaoyong Shen</a>,
                    <a href="https://eecs.pku.edu.cn/info/1502/6732.htm">Sheng Li</a>,
                    <a href="https://eecs.pku.edu.cn/info/1502/6725.htm">Guoping Wang</a>,
                    <a href="http://scholar.google.com/citations?user=nFhLmFkAAAAJ&hl=zh-CN">Yu-Wing Tai</a>
                    <br>
                    <em>CVPR</em>, 2019
                    <br>
                    <!-- <a href="http://jonbarron.info/mipnerf360">project page</a> -->
                    <!-- / -->
                    <a
                      href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Yi_MMFace_A_Multi-Metric_Regression_Network_for_Unconstrained_Face_Reconstruction_CVPR_2019_paper.pdf">arXiv</a>
                    /
                    <!-- <a href="https://youtu.be/zBSH-k9GbV4">video</a> -->
                    <p></p>
                    <!-- <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p> -->
                  </td>
                </tr>

              </tbody>
            </table>


            </div>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody>
                <tr>
                  <td>
                    <heading>Academic Services</heading>
                  </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
              <tbody>

                <tr>
                  <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td> -->
                  <td width="100%" valign="center">
                    <strong>Conference Reviewer:</strong> CVPR, ICCV, ECCV, 3DV, GCPR, NeurIPS, ACMMM.
                    <br>
                    <strong>Journal Reviewer:</strong> Computers & Graphics, IJCV.
                    <br>
                  </td>
                </tr>

              </tbody>
            </table>
            
        </td>
      </tr>
  </table>

  <script src="script.js"></script>

</body>

</html>